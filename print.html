<!DOCTYPE HTML>
<html lang="en" class="sidebar-visible no-js light">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Candle Documentation</title>
        <meta name="robots" content="noindex" />


        <!-- Custom HTML head -->
        
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff" />

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" href="highlight.css">
        <link rel="stylesheet" href="tomorrow-night.css">
        <link rel="stylesheet" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

    </head>
    <body>
    <div id="body-container">
        <!-- Provide site root to javascript -->
        <script>
            var path_to_root = "";
            var default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? "navy" : "light";
        </script>

        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                var theme = localStorage.getItem('mdbook-theme');
                var sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            var theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            var html = document.querySelector('html');
            html.classList.remove('no-js')
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add('js');
        </script>

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            var html = document.querySelector('html');
            var sidebar = null;
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
            }
            html.classList.remove('sidebar-visible');
            html.classList.add("sidebar-" + sidebar);
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <div class="sidebar-scrollbox">
                <ol class="chapter"><li class="chapter-item expanded affix "><a href="index.html">Introduction</a></li><li class="chapter-item expanded affix "><li class="part-title">User Guide</li><li class="chapter-item expanded "><a href="guide/installation.html"><strong aria-hidden="true">1.</strong> Installation</a></li><li class="chapter-item expanded "><a href="guide/hello_world.html"><strong aria-hidden="true">2.</strong> Hello World - MNIST</a></li><li class="chapter-item expanded "><a href="guide/cheatsheet.html"><strong aria-hidden="true">3.</strong> PyTorch cheatsheet</a></li><li class="chapter-item expanded affix "><li class="part-title">Reference Guide</li><li class="chapter-item expanded "><a href="inference/index.html"><strong aria-hidden="true">4.</strong> Running a model</a></li><li><ol class="section"><li class="chapter-item expanded "><a href="inference/hub.html"><strong aria-hidden="true">4.1.</strong> Using the hub</a></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">5.</strong> Error management</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">6.</strong> Advanced Cuda usage</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">6.1.</strong> Writing a custom kernel</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">6.2.</strong> Porting a custom kernel</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">7.</strong> Using MKL</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">8.</strong> Creating apps</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">8.1.</strong> Creating a WASM app</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">8.2.</strong> Creating a REST api webserver</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">8.3.</strong> Creating a desktop Tauri app</div></li></ol></li><li class="chapter-item expanded "><div><strong aria-hidden="true">9.</strong> Training</div></li><li><ol class="section"><li class="chapter-item expanded "><div><strong aria-hidden="true">9.1.</strong> MNIST</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">9.2.</strong> Fine-tuning</div></li><li class="chapter-item expanded "><div><strong aria-hidden="true">9.3.</strong> Serialization</div></li></ol></li></ol>
            </div>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle"></div>
        </nav>

        <!-- Track and set sidebar scroll position -->
        <script>
            var sidebarScrollbox = document.querySelector('#sidebar .sidebar-scrollbox');
            sidebarScrollbox.addEventListener('click', function(e) {
                if (e.target.tagName === 'A') {
                    sessionStorage.setItem('sidebar-scroll', sidebarScrollbox.scrollTop);
                }
            }, { passive: true });
            var sidebarScrollTop = sessionStorage.getItem('sidebar-scroll');
            sessionStorage.removeItem('sidebar-scroll');
            if (sidebarScrollTop) {
                // preserve sidebar scroll position when navigating via links within sidebar
                sidebarScrollbox.scrollTop = sidebarScrollTop;
            } else {
                // scroll sidebar to current active section when navigating via "next/previous chapter" buttons
                var activeSection = document.querySelector('#sidebar .active');
                if (activeSection) {
                    activeSection.scrollIntoView({ block: 'center' });
                }
            }
        </script>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <button id="sidebar-toggle" class="icon-button" type="button" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </button>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search. (Shortkey: s)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="S" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Candle Documentation</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="introduction"><a class="header" href="#introduction">Introduction</a></h1>
<h2 id="features"><a class="header" href="#features">Features</a></h2>
<ul>
<li>Simple syntax, looks and feels like PyTorch.
<ul>
<li>Model training.</li>
<li>Embed user-defined ops/kernels, such as <a href="https://github.com/huggingface/candle/blob/89ba005962495f2bfbda286e185e9c3c7f5300a3/candle-flash-attn/src/lib.rs#L152">flash-attention v2</a>.</li>
</ul>
</li>
<li>Backends.
<ul>
<li>Optimized CPU backend with optional MKL support for x86 and Accelerate for macs.</li>
<li>CUDA backend for efficiently running on GPUs, multiple GPU distribution via NCCL.</li>
<li>WASM support, run your models in a browser.</li>
</ul>
</li>
<li>Model support out of the box.
<ul>
<li>LLMs: Llama v1 and v2, Falcon, StarCoder.</li>
<li>Whisper.</li>
<li>Stable Diffusion.</li>
</ul>
</li>
<li>Serverless (on CPU), small and fast deployments.</li>
</ul>
<p>This book will introduce step by step how to use <code>candle</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="installation"><a class="header" href="#installation">Installation</a></h1>
<p>Start by creating a new app:</p>
<pre><code class="language-bash">cargo new myapp
cd myapp
cargo add --git https://github.com/huggingface/candle.git candle-core
</code></pre>
<p>At this point, candle will be built <strong>without</strong> CUDA support.
To get CUDA support use the <code>cuda</code> feature</p>
<pre><code class="language-bash">cargo add --git https://github.com/huggingface/candle.git candle-core --features cuda
</code></pre>
<p>You can check everything works properly:</p>
<pre><code class="language-bash">cargo build
</code></pre>
<p>You can also see the <code>mkl</code> feature which could be interesting to get faster inference on CPU. <a href="guide/./advanced/mkl.html">Using mkl</a></p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="hello-world"><a class="header" href="#hello-world">Hello world!</a></h1>
<p>We will now create the hello world of the ML world, building a model capable of solving MNIST dataset.</p>
<p>Open <code>src/main.rs</code> and fill in this content:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">extern crate candle_core;
</span>use candle_core::{DType, Device, Result, Tensor};

struct Model {
    first: Tensor,
    second: Tensor,
}

impl Model {
    fn forward(&amp;self, image: &amp;Tensor) -&gt; Result&lt;Tensor&gt; {
        let x = image.matmul(&amp;self.first)?;
        let x = x.relu()?;
        x.matmul(&amp;self.second)
    }
}

fn main() -&gt; Result&lt;()&gt; {
    // Use Device::new_cuda(0)?; to use the GPU.
    let device = Device::Cpu;

    let first = Tensor::zeros((784, 100), DType::F32, &amp;device)?;
    let second = Tensor::zeros((100, 10), DType::F32, &amp;device)?;
    let model = Model { first, second };

    let dummy_image = Tensor::zeros((1, 784), DType::F32, &amp;device)?;

    let digit = model.forward(&amp;dummy_image)?;
    println!(&quot;Digit {digit:?} digit&quot;);
    Ok(())
}</code></pre></pre>
<p>Everything should now run with:</p>
<pre><code class="language-bash">cargo run --release
</code></pre>
<h2 id="using-a-linear-layer"><a class="header" href="#using-a-linear-layer">Using a <code>Linear</code> layer.</a></h2>
<p>Now that we have this, we might want to complexify things a bit, for instance by adding <code>bias</code> and creating
the classical <code>Linear</code> layer. We can do as such</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">extern crate candle_core;
</span><span class="boring">use candle_core::{DType, Device, Result, Tensor};
</span>struct Linear{
    weight: Tensor,
    bias: Tensor,
}
impl Linear{
    fn forward(&amp;self, x: &amp;Tensor) -&gt; Result&lt;Tensor&gt; {
        let x = x.matmul(&amp;self.weight)?;
        x.broadcast_add(&amp;self.bias)
    }
}

struct Model {
    first: Linear,
    second: Linear,
}

impl Model {
    fn forward(&amp;self, image: &amp;Tensor) -&gt; Result&lt;Tensor&gt; {
        let x = self.first.forward(image)?;
        let x = x.relu()?;
        self.second.forward(&amp;x)
    }
}
<span class="boring">}</span></code></pre></pre>
<p>This will change the model running code into a new function</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">extern crate candle_core;
</span><span class="boring">use candle_core::{DType, Device, Result, Tensor};
</span><span class="boring">struct Linear{
</span><span class="boring">    weight: Tensor,
</span><span class="boring">    bias: Tensor,
</span><span class="boring">}
</span><span class="boring">impl Linear{
</span><span class="boring">    fn forward(&amp;self, x: &amp;Tensor) -&gt; Result&lt;Tensor&gt; {
</span><span class="boring">        let x = x.matmul(&amp;self.weight)?;
</span><span class="boring">        x.broadcast_add(&amp;self.bias)
</span><span class="boring">    }
</span><span class="boring">}
</span><span class="boring">
</span><span class="boring">struct Model {
</span><span class="boring">    first: Linear,
</span><span class="boring">    second: Linear,
</span><span class="boring">}
</span><span class="boring">
</span><span class="boring">impl Model {
</span><span class="boring">    fn forward(&amp;self, image: &amp;Tensor) -&gt; Result&lt;Tensor&gt; {
</span><span class="boring">        let x = self.first.forward(image)?;
</span><span class="boring">        let x = x.relu()?;
</span><span class="boring">        self.second.forward(&amp;x)
</span><span class="boring">    }
</span><span class="boring">}
</span>fn main() -&gt; Result&lt;()&gt; {
    // Use Device::new_cuda(0)?; to use the GPU.
    // Use Device::Cpu; to use the CPU.
    let device = Device::cuda_if_available(0)?;

    // Creating a dummy model
    let weight = Tensor::zeros((784, 100), DType::F32, &amp;device)?;
    let bias = Tensor::zeros((100, ), DType::F32, &amp;device)?;
    let first = Linear{weight, bias};
    let weight = Tensor::zeros((100, 10), DType::F32, &amp;device)?;
    let bias = Tensor::zeros((10, ), DType::F32, &amp;device)?;
    let second = Linear{weight, bias};
    let model = Model { first, second };

    let dummy_image = Tensor::zeros((1, 784), DType::F32, &amp;device)?;

    // Inference on the model
    let digit = model.forward(&amp;dummy_image)?;
    println!(&quot;Digit {digit:?} digit&quot;);
    Ok(())
}</code></pre></pre>
<p>Now it works, it is a great way to create your own layers.
But most of the classical layers are already implemented in <a href="https://github.com/huggingface/candle/tree/main/candle-nn">candle-nn</a>.</p>
<h2 id="using-candle_nn"><a class="header" href="#using-candle_nn">Using <code>candle_nn</code>.</a></h2>
<p>For instance <a href="https://github.com/huggingface/candle/blob/main/candle-nn/src/linear.rs">Linear</a> is already there.
This Linear is coded with PyTorch layout in mind, to reuse better existing models out there, so it uses the transpose of the weights and not the weights directly.</p>
<p>So instead we can simplify our example:</p>
<pre><code class="language-bash">cargo add --git https://github.com/huggingface/candle.git candle-nn
</code></pre>
<p>And rewrite our examples using it</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">extern crate candle_core;
</span><span class="boring">extern crate candle_nn;
</span>use candle_core::{DType, Device, Result, Tensor};
use candle_nn::Linear;

struct Model {
    first: Linear,
    second: Linear,
}

impl Model {
    fn forward(&amp;self, image: &amp;Tensor) -&gt; Result&lt;Tensor&gt; {
        let x = self.first.forward(image)?;
        let x = x.relu()?;
        self.second.forward(&amp;x)
    }
}

fn main() -&gt; Result&lt;()&gt; {
    // Use Device::new_cuda(0)?; to use the GPU.
    let device = Device::Cpu;

    // This has changed (784, 100) -&gt; (100, 784) !
    let weight = Tensor::zeros((100, 784), DType::F32, &amp;device)?;
    let bias = Tensor::zeros((100, ), DType::F32, &amp;device)?;
    let first = Linear::new(weight, Some(bias));
    let weight = Tensor::zeros((10, 100), DType::F32, &amp;device)?;
    let bias = Tensor::zeros((10, ), DType::F32, &amp;device)?;
    let second = Linear::new(weight, Some(bias));
    let model = Model { first, second };

    let dummy_image = Tensor::zeros((1, 784), DType::F32, &amp;device)?;

    let digit = model.forward(&amp;dummy_image)?;
    println!(&quot;Digit {digit:?} digit&quot;);
    Ok(())
}</code></pre></pre>
<p>Feel free to modify this example to use <code>Conv2d</code> to create a classical convnet instead.</p>
<p>Now that we have the running dummy code we can get to more advanced topics:</p>
<ul>
<li><a href="guide/./guide/cheatsheet.html">For PyTorch users</a></li>
<li><a href="guide/./inference/README.html">Running existing models</a></li>
<li><a href="guide/./training/README.html">Training models</a></li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="pytorch-cheatsheet"><a class="header" href="#pytorch-cheatsheet">Pytorch cheatsheet</a></h1>
<p>Cheatsheet:</p>
<div class="table-wrapper"><table><thead><tr><th></th><th>Using PyTorch</th><th>Using Candle</th></tr></thead><tbody>
<tr><td>Creation</td><td><code>torch.Tensor([[1, 2], [3, 4]])</code></td><td><code>Tensor::new(&amp;[[1f32, 2.], [3., 4.]], &amp;Device::Cpu)?</code></td></tr>
<tr><td>Creation</td><td><code>torch.zeros((2, 2))</code></td><td><code>Tensor::zeros((2, 2), DType::F32, &amp;Device::Cpu)?</code></td></tr>
<tr><td>Indexing</td><td><code>tensor[:, :4]</code></td><td><code>tensor.i((.., ..4))?</code></td></tr>
<tr><td>Operations</td><td><code>tensor.view((2, 2))</code></td><td><code>tensor.reshape((2, 2))?</code></td></tr>
<tr><td>Operations</td><td><code>a.matmul(b)</code></td><td><code>a.matmul(&amp;b)?</code></td></tr>
<tr><td>Arithmetic</td><td><code>a + b</code></td><td><code>&amp;a + &amp;b</code></td></tr>
<tr><td>Device</td><td><code>tensor.to(device=&quot;cuda&quot;)</code></td><td><code>tensor.to_device(&amp;Device::Cuda(0))?</code></td></tr>
<tr><td>Dtype</td><td><code>tensor.to(dtype=torch.float16)</code></td><td><code>tensor.to_dtype(&amp;DType::F16)?</code></td></tr>
<tr><td>Saving</td><td><code>torch.save({&quot;A&quot;: A}, &quot;model.bin&quot;)</code></td><td><code>candle::safetensors::save(&amp;HashMap::from([(&quot;A&quot;, A)]), &quot;model.safetensors&quot;)?</code></td></tr>
<tr><td>Loading</td><td><code>weights = torch.load(&quot;model.bin&quot;)</code></td><td><code>candle::safetensors::load(&quot;model.safetensors&quot;, &amp;device)</code></td></tr>
</tbody></table>
</div><div style="break-before: page; page-break-before: always;"></div><h1 id="running-a-model"><a class="header" href="#running-a-model">Running a model</a></h1>
<p>In order to run an existing model, you will need to download and use existing weights.
Most models are already available on https://huggingface.co/ in <a href="https://github.com/huggingface/safetensors"><code>safetensors</code></a> format.</p>
<p>Let's get started by running an old model : <code>bert-base-uncased</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="using-the-hub"><a class="header" href="#using-the-hub">Using the hub</a></h1>
<p>Install the <a href="https://github.com/huggingface/hf-hub"><code>hf-hub</code></a> crate:</p>
<pre><code class="language-bash">cargo add hf-hub
</code></pre>
<p>Then let's start by downloading the <a href="https://huggingface.co/bert-base-uncased/tree/main">model file</a>.</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">extern crate candle_core;
</span><span class="boring">extern crate hf_hub;
</span>use hf_hub::api::sync::Api;
use candle_core::Device;

let api = Api::new().unwrap();
let repo = api.model(&quot;bert-base-uncased&quot;.to_string());

let weights = repo.get(&quot;model.safetensors&quot;).unwrap();

let weights = candle_core::safetensors::load(weights, &amp;Device::Cpu);
<span class="boring">}</span></code></pre></pre>
<p>We now have access to all the <a href="https://huggingface.co/bert-base-uncased?show_tensors=true">tensors</a> within the file.</p>
<p>You can check all the names of the tensors <a href="https://huggingface.co/bert-base-uncased?show_tensors=true">here</a></p>
<h2 id="using-async"><a class="header" href="#using-async">Using async</a></h2>
<p><code>hf-hub</code> comes with an async API.</p>
<pre><code class="language-bash">cargo add hf-hub --features tokio
</code></pre>
<pre><code class="language-rust ignore"><span class="boring">This is tested directly in examples crate because it needs external dependencies unfortunately:
</span><span class="boring">See [this](https://github.com/rust-lang/mdBook/issues/706)
</span>use candle::Device;
use hf_hub::api::tokio::Api;

let api = Api::new().unwrap();
let repo = api.model(&quot;bert-base-uncased&quot;.to_string());

let weights_filename = repo.get(&quot;model.safetensors&quot;).await.unwrap();

let weights = candle::safetensors::load(weights_filename, &amp;Device::Cpu).unwrap();</code></pre>
<h2 id="using-in-a-real-model"><a class="header" href="#using-in-a-real-model">Using in a real model.</a></h2>
<p>Now that we have our weights, we can use them in our bert architecture:</p>
<pre><pre class="playground"><code class="language-rust"><span class="boring">#![allow(unused)]
</span><span class="boring">fn main() {
</span><span class="boring">extern crate candle_core;
</span><span class="boring">extern crate candle_nn;
</span><span class="boring">extern crate hf_hub;
</span><span class="boring">use hf_hub::api::sync::Api;
</span><span class="boring">
</span><span class="boring">let api = Api::new().unwrap();
</span><span class="boring">let repo = api.model(&quot;bert-base-uncased&quot;.to_string());
</span><span class="boring">
</span><span class="boring">let weights = repo.get(&quot;model.safetensors&quot;).unwrap();
</span>use candle_core::{Device, Tensor, DType};
use candle_nn::Linear;

let weights = candle_core::safetensors::load(weights, &amp;Device::Cpu).unwrap();

let weight = weights.get(&quot;bert.encoder.layer.0.attention.self.query.weight&quot;).unwrap();
let bias = weights.get(&quot;bert.encoder.layer.0.attention.self.query.bias&quot;).unwrap();

let linear = Linear::new(weight.clone(), Some(bias.clone()));

let input_ids = Tensor::zeros((3, 768), DType::F32, &amp;Device::Cpu).unwrap();
let output = linear.forward(&amp;input_ids).unwrap();
<span class="boring">}</span></code></pre></pre>
<p>For a full reference, you can check out the full <a href="https://github.com/LaurentMazare/candle/tree/main/candle-examples/examples/bert">bert</a> example.</p>
<h2 id="memory-mapping"><a class="header" href="#memory-mapping">Memory mapping</a></h2>
<p>For more efficient loading, instead of reading the file, you could use <a href="https://docs.rs/memmap2/latest/memmap2/"><code>memmap2</code></a></p>
<p><strong>Note</strong>: Be careful about memory mapping it seems to cause issues on <a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/issues/5893">Windows, WSL</a>
and will definitely be slower on network mounted disk, because it will issue more read calls.</p>
<pre><code class="language-rust ignore">use candle::Device;
use hf_hub::api::sync::Api;
use memmap2::Mmap;
use std::fs;

let api = Api::new().unwrap();
let repo = api.model(&quot;bert-base-uncased&quot;.to_string());
let weights_filename = repo.get(&quot;model.safetensors&quot;).unwrap();

let file = fs::File::open(weights_filename).unwrap();
let mmap = unsafe { Mmap::map(&amp;file).unwrap() };
let weights = candle::safetensors::load_buffer(&amp;mmap[..], &amp;Device::Cpu).unwrap();</code></pre>
<p><strong>Note</strong>: This operation is <strong>unsafe</strong>. <a href="https://docs.rs/memmap2/latest/memmap2/struct.Mmap.html#safety">See the safety notice</a>.
In practice model files should never be modified, and the mmaps should be mostly READONLY anyway, so the caveat most likely does not apply, but always keep it in mind.</p>
<h2 id="tensor-parallel-sharding"><a class="header" href="#tensor-parallel-sharding">Tensor Parallel Sharding</a></h2>
<p>When using multiple GPUs to use in Tensor Parallel in order to get good latency, you can load only the part of the Tensor you need.</p>
<p>For that you need to use <a href="https://crates.io/crates/safetensors"><code>safetensors</code></a> directly.</p>
<pre><code class="language-bash">cargo add safetensors
</code></pre>
<pre><code class="language-rust ignore">use candle::{DType, Device, Tensor};
use hf_hub::api::sync::Api;
use memmap2::Mmap;
use safetensors::slice::IndexOp;
use safetensors::SafeTensors;
use std::fs;

let api = Api::new().unwrap();
let repo = api.model(&quot;bert-base-uncased&quot;.to_string());
let weights_filename = repo.get(&quot;model.safetensors&quot;).unwrap();

let file = fs::File::open(weights_filename).unwrap();
let mmap = unsafe { Mmap::map(&amp;file).unwrap() };

// Use safetensors directly
let tensors = SafeTensors::deserialize(&amp;mmap[..]).unwrap();
let view = tensors
    .tensor(&quot;bert.encoder.layer.0.attention.self.query.weight&quot;)
    .unwrap();

// We're going to load shard with rank 1, within a world_size of 4
// We're going to split along dimension 0 doing VIEW[start..stop, :]
let rank = 1;
let world_size = 4;
let dim = 0;
let dtype = view.dtype();
let mut tp_shape = view.shape().to_vec();
let size = tp_shape[0];

if size % world_size != 0 {
    panic!(&quot;The dimension is not divisble by `world_size`&quot;);
}
let block_size = size / world_size;
let start = rank * block_size;
let stop = (rank + 1) * block_size;

// Everything is expressed in tensor dimension
// bytes offsets is handled automatically for safetensors.

let iterator = view.slice(start..stop).unwrap();

tp_shape[dim] = block_size;

// Convert safetensors Dtype to candle DType
let dtype: DType = dtype.try_into().unwrap();

// TODO: Implement from_buffer_iterator so we can skip the extra CPU alloc.
let raw: Vec&lt;u8&gt; = iterator.into_iter().flatten().cloned().collect();
let tp_tensor = Tensor::from_raw_buffer(&amp;raw, dtype, &amp;tp_shape, &amp;Device::Cpu).unwrap();</code></pre>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>




        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>

    </div>
    </body>
</html>
